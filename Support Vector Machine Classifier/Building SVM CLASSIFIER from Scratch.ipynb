{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8452fb-29c1-4cef-8585-186f73296437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Equation of the Hyperplane:\\n\\ny = wx - b\\n\\nGradient Descent:\\n\\nGradient Descent is an optimization algorithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.\\n\\nw = w - α*dw\\n\\nb = b - α*db\\n\\nLearning Rate:\\n\\nLearning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Equation of the Hyperplane:\n",
    "\n",
    "y = wx - b\n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "Gradient Descent is an optimization algorithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.\n",
    "\n",
    "w = w - α*dw\n",
    "\n",
    "b = b - α*db\n",
    "\n",
    "Learning Rate:\n",
    "\n",
    "Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a552bca3-1e42-4d14-b77a-c04e1db40a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dependencies \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ee4153-57c0-4b6f-8e18-74431f97d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Support vector machine classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0ccd192-838c-4127-8c82-bad495c8ccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class svm_classifier(): \n",
    "    #for hyperparamers  self, \n",
    "    def __init__(self, learning_rate , no_of_iteration, lambda_parameter):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.no_of_iteration = no_of_iteration\n",
    "        self.lambda_parameter = lambda_parameter\n",
    "        \n",
    "    #fitting the dataset to svm classifier \n",
    "    def fit(self, X, Y):\n",
    "        #m - no of datapoints -> no of rows \n",
    "        #n- no of input features -> no of columns , no of weights \n",
    "        #X  = no of features \n",
    "        #Y  = no of outcomes  \n",
    "        self.m, self.n= X.shape\n",
    "\n",
    "        #initating the weight and bias value \n",
    "        self.w = np.zeros(self.n)\n",
    "        self.b = 0\n",
    "\n",
    "        self.X=X\n",
    "        self.Y=Y \n",
    "\n",
    "        #implementing grediant descent algorithm for Optimization \n",
    "        for i in range (self.no_of_iteration): \n",
    "            self.update_weights()\n",
    "        \n",
    "        \n",
    "    #function for updating the weight and bias value \n",
    "    def update_weights(self): \n",
    "\n",
    "        #label encoding \n",
    "        y_label = np.where(self.Y <=0, -1, 1) #if Y==0 ,it will convert to -1\n",
    "        \n",
    "        #graidents(dw,db) \n",
    "        for index, x_i in enumerate(self.X):\n",
    "            #enumerate gives particular index value and its data \n",
    "\n",
    "            condition = y_label[index] * (np.dot(x_i,self.w)-self.b) >=1\n",
    "\n",
    "            if(condition==True): \n",
    "\n",
    "                dw = 2 * self.lambda_parameter * self.w\n",
    "                db = 0 \n",
    "            else: \n",
    "                dw = 2 * self.lambda_parameter * self.w - np.dot(x_i,y_label[index])\n",
    "                db = y_label[index]\n",
    "\n",
    "            self.w = self.w - self.learning_rate * dw \n",
    "            self.b = self.b - self.learning_rate * db \n",
    "\n",
    "    #predicting the label for given input value \n",
    "    def predict(self,X): \n",
    "        output = np.dot(X, self.w) - self.b\n",
    "        predicted_labels = np.sign(output)\n",
    "\n",
    "        y_hat = np.where(predicted_labels <= -1, 0 , 1)\n",
    "\n",
    "        return y_hat() \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb73b1-47df-4a93-9cb7-0a04984915c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
